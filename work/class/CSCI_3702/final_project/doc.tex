\documentclass[12pt]{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{float}
\usepackage{epsfig}
\usepackage{tabularx}

\newcommand{\defn}[1]{\textit{#1}}
\newcommand{\mi}[1]{\mathit{#1}}
\newcommand{\mt}[1]{\text{#1}}

\floatstyle{boxed}
\newfloat{Figure}{tbp}{}

\title{Tree-Adjoining Grammars: A Model of Syntax Representation}
\author{Luke Palmer}

\begin{document}
\maketitle
\doublespace

In 1957, Noam Chomsky introduced four formal classes of language with
which to study Human discourse:  \defn{regular} languages,
\defn{context-free} languages, \defn{context-sensitive} languages, and
finally \defn{unrestricted} or \defn{recursively enumerable} languages.
Each class in this list is a subset of the next.  Since their
introduction, these languages have been widely studied in the contexts
of mathematics, linguistics, and computer science.

This article will present another fairly mature class of languages that
has remained out of sight for everyone but computational linguists.
This is the class of languages generated by the \defn{tree-adjoining
grammars}; the so-called \defn{mildly context-sensitive languages}.
This class is important as it allows us to \defn{lexicalize} the
grammars for a natural language.  To lexicalize a grammar means to
associate exactly one word to each reduction rule.

In Section \ref{sec-free-context}, we introduce the widely-studied
context-free grammars, and present their use in natural language as well
as their shortcomings which led to the introduction of tree-rewriting
grammars.  Section \ref{sec-tree-rewriting-grammars} describes
tree-rewriting grammars with a focus on the tree-adjoining grammars.
Finally, Section \ref{sec-child-language-acquisition} presents a link
between the regular, context-free, and tree-adjoining classes (the
``language hierarchy'') and the process by which children acquire
language.

\section{The Rise and Fall of Free Context}
\label{sec-free-context}

A great deal of research has gone into the context-free languages.  They
were first proposed in order to describe human language, but they
quickly took off in the area of computer science, used to process
programming language syntax, network protocols, and natural
language-like commands for games.  They have also been used in natural
language processing, but more powerful techniques have become more
popular in recent years.  Nonetheless, we will focus on their
applicability to natural language, which will give us a good idea why
they had to be phased out.

Informally, a context-free grammar is a set of rules that takes
``rewritable'' symbols eventually into sequences of words in the target
language.  For example, this is a small version of a common context-free
grammar:

\begin{align}
S  &\rightarrow \mathit{NP} \; \mathit{VP}      \tag{1} \\
\mathit{VP} &\rightarrow V \; \mathit{NP}       \tag{2} \\
\mathit{NP} &\rightarrow \text{``the dog''}     \tag{3} \\
\mathit{NP} &\rightarrow \text{``the cat''}     \tag{4} \\
V  &\rightarrow \text{``chased''}               \tag{5}
\end{align}

The capital letters represent rewritable symbols\footnote{\textit{S}:
Sentence; \textit{NP}: Noun Phrase; \textit{VP}: Verb Phrase;
\textit{V}: Verb.}, and the things in quotes represent words in the
target language.  We can see how the string ``the dog chased the cat''
was generated from $S$ (the so-called \defn{start symbol}) in Figure
\ref{dog-cat-deriv}.  The order in which the rewrites are made does not
matter (in the fourth step I could just as well have used rule (5) to
attain ``the dog chased NP'').  Rules may also be applied more than once
during a derivation (in the fourth step I could have used rule (3) again
to attain ``the dog V the dog'').

\begin{Figure}
\begin{tabularx}{\linewidth}{X|X}
S                      & Start symbol \\
NP VP                  & Rule (1) \\
the dog VP             & Rule (3) \\
the dog V NP           & Rule (2) \\
the dog V the cat      & Rule (4) \\
the dog chased the cat & Rule (5) \\
\end{tabularx}
\caption{Derivation of ``the dog chased the cat''}
\label{dog-cat-deriv}
\end{Figure}

It is not very interesting to use a grammar to generate sentences.  Most
of the time you will get nonsense\footnote{But you can be assured that
it will be syntactically correct nonsense!}.  It is also possible to use
a grammar to \textit{match} sentences; i.e. to see if a particular
sentence could have been generated by a given grammar.   But that isn't
very interesting either, by itself.  What is interesting when you
check that a sentence is generated by a grammar is \textit{how} it was
generated.  This process creates a \defn{parse tree} of the sentence.
The parse tree for the derivation in Figure \ref{dog-cat-deriv} can be
seen in Figure \ref{dog-cat-tree}.

\begin{Figure}
\begin{center}
\epsfig{file=dog-cat-tree.eps,height=2in}
\end{center}
\caption{The parse tree for the derivation of ``the dog chased the
cat''}
\label{dog-cat-tree}
\end{Figure}

After experiencing great success in the theory of programming languages,
context-free grammars began to show their weaknesses in natural language
processing.  For instance, they fail at one of the most fundamental
underlying themes in language: agreement.  To ensure that the subject
agrees with the verb in number, you must define two very similar
\textit{VP}s: $\mathit{VP}_\mathit{sg}$ (singular) and
$\mathit{VP}_\mathit{pl}$ (plural).  To ensure that the subject agrees
with the verb in person, you must define three very similar
\textit{VP}s, and to combine the two properties, you \textit{multiply}
the definitions, resulting in \textit{six} almost-duplicated
productions.  This is clearly getting out of hand.

It was disputed and unknown at the time whether all natural languages
were actually context-free.  In 1985, Shieber\cite{Shieber-1985} showed
convincing evidence that Swiss German was not context-free.  This
increased the pressure on academics to come up with a better
representation of natural syntax.  Fortunately for them, one had already
been around for ten years.

\section{Tree-Rewriting Grammars}
\label{sec-tree-rewriting-grammars}

Context-free grammars were all about rewriting strings to (or from) a
single start symbol, and to create a parse tree in the process.
Tree-rewriting grammars instead start with one or more start trees and
rewrite them using rules until one of them matches the input. 

The first form of tree-rewriting grammar was the \defn{tree substitution
grammar}.  Here, the start trees contained \defn{substitutable nodes},
which were childless nodes marked with a root type (for example,
\textit{NP}).  New trees could be derived from old ones by substituting
a tree with the given root type for that node.  This provided the people
who constructed grammars more control over the derived trees, and it was
a generally encouraging development until it was shown that the tree
substitution grammars derive the same set of languages as the
context-free grammars.

Section \ref{subsec-tags} describes the formalism of tree-adjoining
grammars, and section \ref{subsec-feature-tags} describes the
integration of features, a method for controlling what kinds of
substitutions can be made.

\subsection{Tree-Adjoining Grammars}
\label{subsec-tags}

The tree substitution grammars were modified to allow an additional
operation, namely \defn{adjunction}, which formed the
\defn{tree-adjoining grammars} (TAGs) \cite{Joshi-1975}.  In a tree-adjoining grammar, you
again specify a set of initial trees, which correspond roughly to the
different general forms a sentence can take on.  An example base tree
for English is shown in Figure \ref{base-tree-example}.  Then you
specify \defn{auxiliary trees}, which are a bit tricky.  An auxiliary
substitution takes a subtree $S$ and substitutes a subtree $T$, making
$S$ a subtree of $T$.  It is best illustrated by example.  Figure
\ref{adjoining-substitution} shows an auxiliary substitution on the base
tree in Figure \ref{base-tree-example}.  The shown substitution allows
the string ``I taught him a lesson'' to derive ``I really taught him a
lesson''.

TAGs are (finally!) not equivalent to the context-free grammars.  They
are strictly more powerful, generating the set of ``mildly
context-sensitive languages''.  Chomsky's context-sensitive grammars are
still more powerful.  However, TAGs have had much more success in
practice than the context-sensitive grammars because the formalism is
easier to work with.  It is hard to intuit the generated structure of a
context-sensitive grammar just by looking at it, while it is hard to
miss the generated structure of a TAG.  Also, as we will see, TAGs
readily provide a way to encode semantic information.

\begin{Figure}
\begin{center}
\epsfig{file=base-tree-example.eps,height=2in}
\end{center}
\caption{An example of a base tree for English.  The parenthesized
letters after NP refer to the case that declined pronouns would take on:
\textit{n} for \defn{nominative}, eg. ``\textbf{I} gave John the
ball''; \textit{d} for \defn{dative}, eg. ``John gave \textbf{me} the
ball''; \textit{a} for \defn{accusative}, eg.  ``John gave \textbf{me}
to the asylum''.}
\label{base-tree-example}
\end{Figure}

\begin{Figure}
\begin{center}
\epsfig{file=adjoining-substitution.eps,height=2in}
\end{center}
\caption{An auxiliary substitution on the base tree shown in Figure
\ref{base-tree-example}.  The auxiliary tree is on the left, the
resulting tree is on the right.  The star in the auxiliary tree
represents where to put the subtree that is being replaced by this
substitution.}
\label{adjoining-substitution}
\end{Figure}

The tree-adjoining grammars were ultimately introduced in order to
lexicalize any specified grammar.  That is, it is desirable to associate
one rule with each lexeme\footnote{A lexeme is one unit of meaning,
usually (but not always) a single word.  For example, ``to give'' (and
its various inflected forms) is one lexeme, but ``to give up'' is
another.}.  This is because modern linguistic theory unifies to some
degree the lexicon and the syntax, by treating each lexeme as its
meaning together with a ``minisyntax'' for its usage.

\subsection{Feature-based TAGs}
\label{subsec-feature-tags}

TAGs have some of the same problems that were pointed out for
context-free grammars in section \ref{sec-free-context}.  In order to
allow the sentence ``John has seen the ball'' but disallow ``*John seen
the ball'', node types will unnecessarily multiply.  Two solutions were
proposed for this: \defn{Constrained TAGs} and \defn{Feature-based
TAGs}.

Constrained TAGs allow you to specify a finite set of adjunctions that
can occur at a given node, rather than the usual unconstrained operation
where any matching adjunction can occur.  These haven't been widely
successful, and the engineer in me feels uneasy about them, wanting
things to be extensible.  You don't want to have to go back through all
your rules when you add a new auxiliary tree to see if it should or
should not be allowed to be substituted in each place.

Feature-based TAGs (FTAGs), introduced by Vijay-Shanker in 1987
\cite{Vijay-Shanker-1987}, instead allow you to associate two
\defn{feature sets} with each of the nodes, which must unify with each
other at every position in the derived tree.  These are commonly called
the \defn{top} and \defn{bottom} features, which encode properties of
the structure above and below, respectively, the node in question.  When
performing an adjunction of an auxiliary tree $A$ for a source subtree
$S$, $S$'s top feature must unify with $A$'s root's top feature, and
$S$'s bottom feature must unify with $A$'s *'s bottom feature.  Another
way of saying this is that the top feature says what the tree is
expecting in that position, and the bottom feature says what it actually
has.  Adjunctions must be made in order to make those two things the
same.

This formalism allows us to represent the ``seen'' example above simply,
as shown in Figure \ref{tense-features}.  The feature on \textit{VP} in
this example is \texttt{tense} which represents whether it is a tensed
clause (i.e. has a subject).  The initial tree on the left expects a
tensed \textit{VP}, and provides an un-tensed \textit{VP} on which it
must be based.  The reader should verify that all the features unify by
adjoining the auxiliary tree on the right to the \textit{VP} on the
left.  The stars in the auxiliary tree's features mean ``anything'' or
``don't care''.

\begin{Figure}
\begin{center}
\epsfig{file=tense-features.eps,height=2in}
\end{center}
\caption{An example use of feature structures.  A base tree is on the
left, which is not syntactically correct because the \texttt{tense}
features do not unify on \textit{VP}.  An auxiliary tree is on the
right, which can be adjoined to the \textit{VP} on the left to make
it unify.}
\label{tense-features}
\end{Figure}

\section{Child Language Acquisition}
\label{sec-child-language-acquisition}

Now that we have the mathematical formalisms behind these various
classes of languages, we can see how they relate to the human mind.
What follows is a summary, in the layman's terms established in this
introduction, of Robert Frank in \cite{Frank-2000}.

Frank points out a natural ordering in children's language acquisition,
that utterance of sentences of type 1 below precedes by utterance of
type two, which in turn precedes type three:

\begin{enumerate}
\item They're taking a vacuum cleaner to wipe and puppy dog's
running.  (Conjunction)
\item Tell Iris that I wet my bed. (Complementation)
\item I'm calling the man who fixes the door.  (Relativization)
\end{enumerate}

The same sort of ordering applies to children's assignment of correct
meaning to the following sentences \cite{Bloom-1980}:

\begin{enumerate}
\item The sheep that kissed the monkey kissed the rabbit.
\item Cookie Monster tells Grover to jump over the
fence\footnote{Children who could comprehend the previous sentence would
act out that Cookie Monster was the one jumping}. 
\item Cookie Monster touches Grover after jumping over the
fence\footnote{A similar misunderstanding happened here, where children
thought that Grover was jumping over the fence.  Admittedly, this
sentence could be interpreted ambiguously even by adults.}.
\end{enumerate}

We would like to understand why this ordering occurs.  It had been
argued previously that children hadn't yet acquired the proper
grammatical structures, but Frank argues instead that they are not able
to perform certain types of computations.  Recall the tree substitution
grammars (TSGs) introduced in Section \ref{sec-tree-rewriting-grammars},
If you restrict those grammars not to allow recursion, but instead
simply conjunction (forming the NTSGs), then we can explain the first
type of sentences acquired in learning. If the base trees are all
constructed in the manner of Figure \ref{conjunction}, the
misunderstandings in comprehension above can be understood as children's
use of ``that'' as a synonym for ``and'' (which was shown by Bloom et
al.).  The reader should verify that the sentence ``John has eaten an
apple and Fred has eaten peaches and a candy bar'' fits into this
structure, but that ``John was saying he has eaten an apple'' would
require recursion.

\begin{Figure}
\begin{center}
\epsfig{file=conjunction.eps,height=2in}
\end{center}
\caption{A base tree using conjunction rather than recursion.  The plus
after each symbol means that it can in fact be one or more such symbols
joined with ``and''.}
\label{conjunction}
\end{Figure}

Allowing recursion (the TSGs), we arrive at the second type of sentences
here, but prepositional phrases still cannot be
handled\footnote{Lexically, that is.  Recall our requirement to
associate exactly one lexeme with each tree.}.  Finally, allowing
adjunctions (the TAGs), we can generate an adult language.  Frank's
showed that the NTSGs are equivalent in generative capacity to the
regular languages.  Recall that TSGs are equivalent to the context-free
languages and that TAGs form a subset of the context-sensitive language.
Thus child language acquisition can be modeled by Chomsky's language
hierarchy!

This is quite an incredible finding.  It suggests that there is some
mathematical machinery being developed in a child's brain as it matures,
rather than just acquiring new vocabulary and word uses.  This finding
forms yet another argument for the pervasive lexicon; that is, if we
drop the requirement to lexicalize the grammar, then we can represent
most sentences as context-free, and many as regular, and there would be
no such necessary ordering.

But how, then, do we explain the fact that Figure \ref{conjunction} is
not lexicalized?  Elman \cite{Elman-1990} explains that as children are
learning, they treat combinations of words as single lexical units.
That is, ``has-eaten'' and ``ate'' are learned synonyms rather than
inflections.

However, if such computational development accurately represents our
process of learning language, it seems that our ability to utter and
comprehend certain types of sentences would be quantized; all sentences
of a certain type would emerge simultaneously.  Though this has not been
studied\footnote{Read: I am not aware of a study...}, it doesn't seem
very likely.  Frank conjectures that ``the more powerful tree rewriting
mechanisms are unavailable simply because they demand too much of the
child's resources, [and] they could become available when other
resources are freed.''  This claim is supported by Joshi's complexity
metrics \cite{Joshi-1990} for tree transformations.

Cognitive scientists keep finding more about the brain's ability to make
generalizations.  I claim that it is the linguistic center that is in
charge of making such abstractions.  Mathematics, the most abstract and
symbolic field, is simply a transformation language.  Before a
mathematical idea is widely accepted, a suggestive and clear
notation---even a linguistic metaphor---must be created for it.  It is
possible to think of all our abstract thought as computations similar to
the tree transformations we have seen in this article.  If so, then
truly it is just the ability to use language that separates humans from
the other animals on the Earth.

\clearpage

\singlespace
\begin{thebibliography}{4in}
\bibitem{Abeille-2000} Abeill\'e, Anne and Rambow, Owen.  2000.  Tree
Adjoining Grammar: An Overview.  \textit{CSLI Lecture Notes no. 107}:
1--68.
\bibitem{Bloom-1980} Bloom, Lois, Margaret Lahey, Lois Hood, Karin
Lifter, and Kathleen Fiess.  1980.  Complex sentences: Acquisition of
syntactic connectives and the semantics relations they encode.
\textit{Journal of Child Language 7}: 235--261.
\bibitem{Elman-1990} Elman, Jeffrey L.  1990.  Finding Structure in
Time.  \textit{Cognitive Science 14}: 179--211.
\bibitem{Frank-2000} Frank, Robert.  2000.  From Regular to Context-Free
to Mildly Context-Sensitive Systems: The Path of Child Language
Acquisition.  \textit{CSLI Lecture Notes no. 107}: 101--120.
\bibitem{Gardent-2005} Gardent, Claire and Parmentier, Yannick.  2005.
Large Scale Semantic Construction for Tree Adjoining Grammars.
\textit{Logical Aspects of Computational Lingusitics 2005}: 131--146.
\bibitem{Joshi-1975} Joshi, Aravind K., Leon Levy, and M. Takahashi.
1975. Tree Adjunct Grammars. \textit{Journal of Computer and System
Sciences 10}: 136--163.
\bibitem{Joshi-1990} Joshi, Aravind K.  Processing Crossed and Nested
Dependencies: an Automaton Perspective on Psycholinguistic Results.
\textit{Language and Cognitive Processes 5}.
\bibitem{Jurafsky-2000} Jurafsky, Daniel and Martin, James H. 2000.
\textit{Speech and Language Processing}.  Prentice Hall, NJ.
\bibitem{Shieber-1985} Shieber, Stuart B.  1985. Evidence against the
context-freeness of natural language.  \textit{Linguistics and
Philosophy 8}: 333--343.
\bibitem{Vijay-Shanker-1987} Vijay-Shanker, K. 1987.  \textit{A study of
Tree Adjoining Grammars}.  Doctoral dissertation, Department of Computer
and Information Science, University of Pennsylvania, Philadelphia, PA,
December.
\end{thebibliography}

\end{document}

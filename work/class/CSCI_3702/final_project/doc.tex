\documentclass[12pt]{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{float}
\usepackage{epsfig}
\usepackage{tabularx}

\newcommand{\defn}[1]{\textit{#1}}
\newcommand{\XXX}[1]{\textbf{XXX #1}}

\floatstyle{boxed}
\newfloat{Figure}{tbp}{}

\title{Tree-Adjoining Grammars: A Model of Syntax Representation}
\author{Luke Palmer}

\begin{document}
\maketitle
\doublespace

$<<$Introduction$>>$

\section{The Rise and Fall of Free Context}
\label{sec-free-context}

In 1957, Noam Chomsky introduced four formal classes of languages with
which to study Human language:  \defn{regular} languages,
\defn{context-free} languages, \defn{context-sensitive} languages,
and \defn{recursively enumerable} languages.  Each class in this list
is a subset of the next.  Most of the research effort concentrated on
grammars for the context-free languages, since they seemed to be the
best fit for both natural and computer languages. \XXX{Need a
Better Reason.}

Informally, a context-free grammar is a set of rewrite rules that take
``rewritable'' symbols eventually into sequences of words in the target
language.  For example, this is a small version of a common context-free
grammar:

\begin{align}
S  &\rightarrow \mathit{NP} \; \mathit{VP}      \tag{1} \\
\mathit{VP} &\rightarrow V \; \mathit{NP}       \tag{2} \\
\mathit{NP} &\rightarrow \text{``the dog''}     \tag{3} \\
\mathit{NP} &\rightarrow \text{``the cat''}     \tag{4} \\
V  &\rightarrow \text{``chased''}               \tag{5}
\end{align}

The capital letters represent rewritable symbols, and the things in
quotes represent words in the target language.  We can see how the
string ``the dog chased the cat'' was generated from $S$ (the so-called
\defn{start symbol}) in Figure \ref{dog-cat-deriv}.  The order in
which the rewrites are made does not matter (in the fourth step I could
just have well used rule (5) to attain ``the dog chased NP'').  Rules may
also be applied more than once during a derivation (in the fourth step I
could have used rule (3) again to attain ``the dog V the dog'').

\begin{Figure}
\begin{tabularx}{\linewidth}{X|X}
S                      & Start symbol \\
NP VP                  & Rule (1) \\
the dog VP             & Rule (3) \\
the dog V NP           & Rule (2) \\
the dog V the cat      & Rule (4) \\
the dog chased the cat & Rule (5) \\
\end{tabularx}
\caption{Derivation of ``the dog chased the cat''}
\label{dog-cat-deriv}
\end{Figure}

It is not very interesting to use a grammar to generate sentences.  Most
of the time you will get nonsense\footnote{But you can be assured that
it will be syntactically correct nonsense!}.  It is also possible to use
a grammar to \textit{match} sentences; i.e. to see if a particular
sentence could have been generated by a given grammar.   But that isn't
very interesting either, by itself.  What is interesting when you
check that a sentence is generated by a grammar is \textit{how} it was
generated.  This process creates a \defn{parse tree} of the sentence.
The parse tree for the derivation in Figure \ref{dog-cat-deriv} can be
seen in Figure \ref{dog-cat-tree}.

\begin{Figure}
\epsfig{file=dog-cat-tree.eps,width=3in}
\caption{The parse tree for the derivation of ``the dog chased the
cat''}
\label{dog-cat-tree}
\end{Figure}

After experiencing great success in the theory of programming languages,
context-free grammars began to show their weaknesses in natural language
processing.  For instance, they fail at one of the most fundamental
underlying themes in language: agreement.  To ensure that the subject
agrees with the verb in number, you must define two very similar
\textit{VP}s: $\mathit{VP}_\mathit{sg}$ (singular) and
$\mathit{VP}_\mathit{pl}$ (plural).  To ensure that the subject agrees
with the verb in person, you must define three very similar
\textit{VP}s, and to combine the two properties, you \textit{multiply}
the definitions, resulting in \textit{six} almost-duplicated
productions.  This is clearly getting out of hand.

It was disputed and unknown at the time whether all natural languages
were actually context-free.  In 1985, Shieber\cite{Shieber-1985} showed
convincing evidence that Swiss German was not context-free.  This
increased the pressure on academics to come up with a better
representation of natural syntax.  Fortunately for them, one had already
been around for ten years.

\section{Tree-Rewriting Grammars}

Context-free grammars were all about rewriting strings to (or from) a
single start symbol, and to create a parse tree in the process.
Tree-rewriting grammars instead start with one or more start trees and
rewrite them using rules until one of them matches the input. 

The first form of tree-rewriting grammar was the \defn{tree substitution
grammar}.  Here, the start trees contained \defn{substitutable nodes},
which were childless nodes marked with a root type (for example,
\textit{NP}).  New trees could be derived from old ones by substituting
a tree with the given root type in for that node.  This provided the
people who constructed grammars more control over the derived trees and
was a generally encouraging development until it was shown that the tree
substitution grammars derive the same set of languages as the
context-free grammars.

\subsection{Tree-Adjoining Grammars}

The tree substitution grammars were modified to allow an additional
operation, namely \defn{adjunction}, which formed the
\defn{tree-adjoining grammars} (TAGs) \cite{Joshi-1975}.  In a tree-adjoining grammar, you
again specify a set of initial trees, which correspond roughly to the
different general forms a sentence can take on.  An example base tree
for English is shown in Figure \ref{base-tree-example}.  Then you
specify \defn{auxiliary trees}, which are a bit tricky.  An auxiliary
substitution takes a subtree $S$ and substitutes a subtree $T$, making
$S$ a subtree of $T$.  It is best illustrated by example.  Figure
\ref{adjoining-substitution} shows an auxiliary substitution on the base
tree in Figure \ref{base-tree-example}.  The shown substitution allows
the string ``I taught him a lesson'' to derive ``I really taught him a
lesson''.

TAGs are (finally!) not equivalent to the context-free grammars.  They
are strictly more powerful, generating the set of ``mildly
context-sensitive languages''.  Chomsky's context-sensitive grammars are
still more powerful.  However, TAGs have had much more success in
practice than the context-sensitive grammars because the formalism is
easier to work with.  It is hard to intuit the generated structure of a
context-sensitive grammar just by looking at it, while it is hard to
miss the generated structure of a TAG.  Also, as we will see, TAGs
readily provide a way to encode semantic information.

\begin{Figure}
\begin{center}
\epsfig{file=base-tree-example.eps,width=3in}
\end{center}
\caption{An example of a base tree for English.  The parenthesized
letters after NP refer to the case that declined pronouns would take on:
\textit{n} for \defn{nominative}, eg. ``\textbf{I} gave John the
Ball''; \textit{d} for \defn{dative}, eg. ``John gave \textbf{me} the
Ball''; \textit{a} for \defn{accusative}, eg.  ``John gave \textbf{me}
to the asylum''.}
\label{base-tree-example}
\end{Figure}

\begin{Figure}
\begin{center}
\epsfig{file=adjoining-substitution.eps,width=5in}
\end{center}
\caption{An auxiliary substitution on the base tree shown in Figure
\ref{base-tree-example}.  The auxiliary tree is on the left, the
resulting tree is on the right.  The star in the auxiliary tree
represents where to put the subtree that is being replaced by this
substitution.}
\label{adjoining-substitution}
\end{Figure}

The tree-adjoining grammars were ultimately introduced in order to
\defn{lexicalize} any specified grammar.  That is, it is desirable to
associate one rule with each lexeme\footnote{A lexeme is one unit of
meaning, usually (but not always) a single word.  For example, ``to
give'' (and its various inflected forms) is one lexeme, but ``to give
up'' is another.}.  This is because modern linguistic theory unifies to
some degree the lexicon and the syntax, by treating each lexeme as its
meaning together with a ``minisyntax'' for its usage.

\subsection{Feature-based TAGs}

TAGs have some of the same problems that were pointed out for
context-free grammars in section \ref{sec-free-context}.  In order to
allow the sentence ``John has seen the ball'' but disallow ``*John seen
the ball'', node types will unnecessarily multiply.  Two solutions were
proposed for this: \defn{Constrained TAGs} and \defn{Feature-based
TAGs}.

Constrained TAGs allow you to specify a finite set of adjunctions that
can occur at a given node, rather than the usual unconstrained operation
where any matching adjunction can occur.  These haven't been widely
successful, and the Engineer in me feels uneasy about them, wanting
things to be extensible.  You don't want to have to go back through all
your rules when you add a new auxiliary tree to see if it should or
should not be allowed to be substituted in each place.

Feature-based TAGs (FTAGs), introduced by Vijay-Shanker in 1987
\cite{Vijay-Shanker-1987}, instead allow you to associate two
\defn{feature sets} with each of the nodes, which must unify with each
other position in the derived tree.

\begin{Figure}
\begin{center}
\epsfig{file=tense-features.eps,width=5in}
\end{center}
\caption{An example use of feature structures.  A base tree is on the
left, which is not syntactically correct because the \texttt{tense}
features do not unify on \textit{VP}.  An auxiliary tree is on the
right, which can be subtituted for the \textit{VP} on the left to make
it unify.}
\end{Figure}

\clearpage

\singlespace
\begin{thebibliography}{4in}
\bibitem{Abeille-2000} Abeill\'e, Anne and Rambow, Owen.  2000.  Tree
Adjoining Grammar: An Overview.  \textit{CSLI Lecture Notes no. 107}:
1--68.
\bibitem{Frank-2000} Frank, Robert.  2000.  From Regular to Context-Free
to Mildly Context-Sensitive Systems: The Path of Child Language
Acquisition.  \textit{CSLI Lecture Notes no. 107}: 101--120.
\bibitem{Joshi-1975} Joshi, Aravind K., Leon Levy, and M. Takahashi.
1975. Tree Adjunct Grammars. \textit{Journal of Computer and System
Sciences 10}: 136--163.
\bibitem{Shieber-1985} Shieber, Stuart B.  1985. Evidence against the
context-freeness of natural language.  \textit{Linguistics and
Philosophy 8}: 333--343.  Through \cite{Abeille-2000}.
\bibitem{Vijay-Shanker-1987} Vijay-Shanker, K. 1987.  \textit{A study of
Tree Ajoining Grammars}.  Doctoral dissertation, Department of Computer
and Information Science, University og Pennsylvania, Philadelphia, PA,
December.  Through \cite{Abeille-2000}.
\end{thebibliography}

\end{document}

\documentclass[12pt]{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{float}
\usepackage{epsfig}
\usepackage{tabularx}

\floatstyle{boxed}
\newfloat{Figure}{tbp}{}

\title{Tree-Adjoining Grammars: A Model of Syntax Representation}
\author{Luke Palmer}

\begin{document}
\maketitle
\doublespace

$<<$Introduction$>>$

\section{The Rise and Fall of Free Context}

In 1957, Noam Chomsky introduced four formal classes of languages with
which to study Human language:  \textit{regular} languages,
\textit{context-free} languages, \textit{context-sensitive} languages,
and \textit{recursively enumerable} languages.  Each class in this list
is a subset of the next.  Most of the research effort concentrated on
grammars for the context-free languages, since they seemed to be the
best fit for both natural and computer languages. \textbf{XXX} Need a
Better Reason.

Informally, a context-free grammar is a set of rewrite rules that take
``rewritable'' symbols eventually into sequences of words in the target
language.  For example, this is a small version of a common context-free
grammar:

\begin{align}
S  &\rightarrow \mathit{NP} \; \mathit{VP}      \tag{1} \\
\mathit{VP} &\rightarrow V \; \mathit{NP}       \tag{2} \\
\mathit{NP} &\rightarrow \text{``the dog''}     \tag{3} \\
\mathit{NP} &\rightarrow \text{``the cat''}     \tag{4} \\
V  &\rightarrow \text{``chased''}               \tag{5}
\end{align}

The capital letters represent rewritable symbols, and the things in
quotes represent words in the target language.  We can see how the
string ``the dog chased the cat'' was generated from $S$ (the so-called
\textit{start symbol}) in Figure \ref{dog-cat-deriv}.  The order in
which the rewrites are made does not matter (in the fourth step I could
just have well used rule (5) to attain ``the dog chased NP'').  Rules may
also be applied more than once during a derivation (in the fourth step I
could have used rule (3) again to attain ``the dog V the dog'').

\begin{Figure}
\begin{tabularx}{\linewidth}{X|X}
S                      & Start symbol \\
NP VP                  & Rule (1) \\
the dog VP             & Rule (3) \\
the dog V NP           & Rule (2) \\
the dog V the cat      & Rule (4) \\
the dog chased the cat & Rule (5) \\
\end{tabularx}
\caption{Derivation of ``the dog chased the cat''}
\label{dog-cat-deriv}
\end{Figure}

It is not very interesting to use a grammar to generate sentences.  Most
of the time you will get nonsense\footnote{But you can be assured that
it will be syntactically correct nonsense!}.  It is also possible to use
a grammar to \textit{match} sentences; i.e. to see if a particular
sentence could have been generated by a given grammar.   But that isn't
very interesting either, by itself.  What is interesting when you
check that a sentence is generated by a grammar is \textit{how} it was
generated.  This process creates a \textit{parse tree} of the sentence.
The parse tree for the derivation in Figure \ref{dog-cat-deriv} can be
seen in Figure \ref{dog-cat-tree}.

\begin{Figure}
\epsfig{file=dog-cat-tree.eps,width=3in}
\caption{The parse tree for the derivation of ``the dog chased the
cat''}
\label{dog-cat-tree}
\end{Figure}

After experiencing great success in the theory of programming lanugages,
context-free grammars began to show their weaknesses in natural language
processing.  For instance, they fail at one of the most fundamental
underlying themes in language: agreement.  To ensure that the subject
agrees with the verb in number, you must define two very similar
\textit{VP}s: $\mathit{VP}_\mathit{sg}$ (singular) and
$\mathit{VP}_\mathit{pl}$ (plural).  To ensure that the subject agrees
with the verb in person, you must define three very similar
\textit{VP}s, and to combine the two properties, you \textit{multiply}
the definitions, resulting in \textit{six} almost-duplicated
productions.  This is clearly getting out of hand.

It was disputed and unknown at the time whether all natural languages
were actually context-free.  In 1985, Shieber showed convincing evidence
that Swiss German was not context-free.  This increased the pressure on
academics to come up with a better representation of natural syntax.
Fortunately for them, one had already been around for ten years.

\section{Tree-Adjoining Grammars}

\end{document}

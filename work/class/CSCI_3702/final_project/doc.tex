\documentclass[12pt]{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{float}
\usepackage{epsfig}
\usepackage{tabularx}

\newcommand{\defn}[1]{\textit{#1}}
\newcommand{\XXX}[1]{\textbf{XXX #1}}

\floatstyle{boxed}
\newfloat{Figure}{tbp}{}

\title{Tree-Adjoining Grammars: A Model of Syntax Representation}
\author{Luke Palmer}

\begin{document}
\maketitle
\doublespace

$<<$Introduction$>>$

\section{The Rise and Fall of Free Context}

In 1957, Noam Chomsky introduced four formal classes of languages with
which to study Human language:  \defn{regular} languages,
\defn{context-free} languages, \defn{context-sensitive} languages,
and \defn{recursively enumerable} languages.  Each class in this list
is a subset of the next.  Most of the research effort concentrated on
grammars for the context-free languages, since they seemed to be the
best fit for both natural and computer languages. \XXX{Need a
Better Reason.}

Informally, a context-free grammar is a set of rewrite rules that take
``rewritable'' symbols eventually into sequences of words in the target
language.  For example, this is a small version of a common context-free
grammar:

\begin{align}
S  &\rightarrow \mathit{NP} \; \mathit{VP}      \tag{1} \\
\mathit{VP} &\rightarrow V \; \mathit{NP}       \tag{2} \\
\mathit{NP} &\rightarrow \text{``the dog''}     \tag{3} \\
\mathit{NP} &\rightarrow \text{``the cat''}     \tag{4} \\
V  &\rightarrow \text{``chased''}               \tag{5}
\end{align}

The capital letters represent rewritable symbols, and the things in
quotes represent words in the target language.  We can see how the
string ``the dog chased the cat'' was generated from $S$ (the so-called
\defn{start symbol}) in Figure \ref{dog-cat-deriv}.  The order in
which the rewrites are made does not matter (in the fourth step I could
just have well used rule (5) to attain ``the dog chased NP'').  Rules may
also be applied more than once during a derivation (in the fourth step I
could have used rule (3) again to attain ``the dog V the dog'').

\begin{Figure}
\begin{tabularx}{\linewidth}{X|X}
S                      & Start symbol \\
NP VP                  & Rule (1) \\
the dog VP             & Rule (3) \\
the dog V NP           & Rule (2) \\
the dog V the cat      & Rule (4) \\
the dog chased the cat & Rule (5) \\
\end{tabularx}
\caption{Derivation of ``the dog chased the cat''}
\label{dog-cat-deriv}
\end{Figure}

It is not very interesting to use a grammar to generate sentences.  Most
of the time you will get nonsense\footnote{But you can be assured that
it will be syntactically correct nonsense!}.  It is also possible to use
a grammar to \textit{match} sentences; i.e. to see if a particular
sentence could have been generated by a given grammar.   But that isn't
very interesting either, by itself.  What is interesting when you
check that a sentence is generated by a grammar is \textit{how} it was
generated.  This process creates a \defn{parse tree} of the sentence.
The parse tree for the derivation in Figure \ref{dog-cat-deriv} can be
seen in Figure \ref{dog-cat-tree}.

\begin{Figure}
\epsfig{file=dog-cat-tree.eps,width=3in}
\caption{The parse tree for the derivation of ``the dog chased the
cat''}
\label{dog-cat-tree}
\end{Figure}

After experiencing great success in the theory of programming lanugages,
context-free grammars began to show their weaknesses in natural language
processing.  For instance, they fail at one of the most fundamental
underlying themes in language: agreement.  To ensure that the subject
agrees with the verb in number, you must define two very similar
\textit{VP}s: $\mathit{VP}_\mathit{sg}$ (singular) and
$\mathit{VP}_\mathit{pl}$ (plural).  To ensure that the subject agrees
with the verb in person, you must define three very similar
\textit{VP}s, and to combine the two properties, you \textit{multiply}
the definitions, resulting in \textit{six} almost-duplicated
productions.  This is clearly getting out of hand.

It was disputed and unknown at the time whether all natural languages
were actually context-free.  In 1985, Shieber\cite{Shieber-1985} showed
convincing evidence that Swiss German was not context-free.  This
increased the pressure on academics to come up with a better
representation of natural syntax.  Fortunately for them, one had already
been around for ten years.

\section{Tree-Adjoining Grammars}

Context-free grammars were all about rewriting strings to (or from) a
single start symbol, and to create a parse tree in the process.
Tree-rewriting grammars instead start with one or more start trees and
rewrite them using rules until one of them matches the input. 

\textbf{XXX} Tree Substitution Grammars?

The tree-adjoining grammars were ultimately introduced in order to
\defn{lexicalize} any specified grammar.  That is, it is desirable to
associate one rule with each lexeme\footnote{A lexeme is one unit of
meaning, usually (but not always) a single word.  For example, ``to
give'' (and its various inflected forms) is one lexeme, but ``to give
up'' is another.}.  This is because modern linguistic theory unifies to
some degree the lexicon and the syntax, by treating each lexeme as its
meaning together with a ``minisyntax'' for its usage.

\begin{Figure}
\begin{center}
\epsfig{file=base-tree-example.eps,width=3in}
\end{center}
\caption{An example of a base tree for English.  The parenthesized
letters after NP refer to the case that declined pronouns would take on:
\textit{n} for \defn{nominative}, eg. ``\textbf{I} gave John the
Ball''; \textit{d} for \defn{dative}, eg. ``John gave \textbf{me} the
Ball''; \textit{a} for \defn{accusative}, eg.  ``John gave \textbf{me}
to the asylum''.}
\label{base-tree-example}
\end{Figure}

In a tree-adjoining grammar, you specify some set of \defn{base
trees}, which correspond roughly to the different general forms a
sentence can take on.  An example base tree for english is shown in
Figure \ref{base-tree-example}.  Then you specify \defn{adjoining
substitutions}, which are a bit tricky.  An adjoining substitution takes
a subtree S and substitutes a subtree T, making S a subtree of T.  It is
best illustrated by example.  Figure \ref{adjoining-substitution} shows
an adjoining substitution on the base tree in Figure
\ref{base-tree-example}.

\begin{Figure}
\begin{center}
\epsfig{file=adjoining-substitution.eps,width=5in}
\end{center}
\caption{An adjoining substitution on the base tree shown in Figure
\ref{base-tree-example}.  The substitution rule is on the left, the
resulting tree is on the right.  The star in the substitution rule
represents where to put the subtree that is being replaced by this
substitution.}
\label{adjoining-substitution}
\end{Figure}

\clearpage

\singlespace
\begin{thebibliography}{4in}
\bibitem{Abeille-2000} Abeill\'e, Anne and Rambow, Owen.  2000.  Tree
Adjoining Grammar: An Overview.  \textit{CSLI Lecture Notes no. 107}:
1--68.
\bibitem{Frank-2000} Frank, Robert.  2000.  From Regular to Context-Free
to Mildly Context-Sensitive Systems: The Path of Child Language
Acquisition.  \textit{CSLI Lecture Notes no. 107}: 101--120.
\bibitem{Shieber-1985} Shieber, Stuart B.  1985. Evidence against the
context-freeness of natural language.  \textit{Linguistics and
Philosophy 8}: 333--343.  Through \cite{Abeille-2000}.
\end{thebibliography}

\end{document}
